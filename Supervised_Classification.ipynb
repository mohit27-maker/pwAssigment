{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhuBqTh6kVXFPf0+yggC3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohit27-maker/pwAssigment/blob/main/Supervised_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "= Information Gain measures how much uncertainty in the target variable is reduced after splitting the data based on a feature. It is used in Decision Trees to select the best attribute for splitting at each node. The formula is:\n",
        "\n",
        " IG(S,A)=Entropy(S)‚àí‚àë‚à£Sv‚à£/‚à£S‚Äã‚à£‚Äã√óEntropy(Sv‚Äã)\n",
        "\n",
        "> Where:\n",
        "- S : the current dataset (before the split)\n",
        "- A : the attribute we are testing\n",
        "- Sv : subset of S where attribute A has value\n",
        "- ‚à£ùëÜùë£‚à£/‚à£ùëÜ‚à£ : proportion of samples in subset\n",
        "- Entropy(S) : impurity (or disorder) of the dataset ùëÜ\n",
        "\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "= Gini Impurity and Entropy both measure the impurity of a dataset in Decision Trees.\n",
        "- Gini Impurity measures how often a randomly chosen element would be incorrectly labeled.\n",
        "- Entropy measures the level of uncertainty or randomness.\n",
        "- Gini is faster to compute and often used in CART (Classification and Regression Trees).\n",
        "- Entropy gives more weight to less probable classes and is used in ID3/C4.5 algorithms.\n",
        "- In practice, both give similar results, but Gini is preferred for speed.\n",
        "\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "= Pre-pruning (also called early stopping) is a technique used to stop a Decision Tree from growing too deep and overfitting the data. It involves setting conditions like maximum depth, minimum samples per node, or minimum information gain before splitting. If these conditions are not met, the split is stopped. This helps create a simpler, faster, and more generalizable model.\n"
      ],
      "metadata": {
        "id": "WqVYUT9OGB1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': clf.feature_importances_\n",
        "})\n",
        "\n",
        "print(feature_importances.sort_values(by='Importance', ascending=False))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q1r_yLrL-n7",
        "outputId": "17688426-0a11-4854-c541-f55799fdd8eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "= A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the best boundary (hyperplane) that separates different classes with the maximum margin. The data points closest to this boundary are called support vectors. SVM can also handle non-linear data using kernel functions (like RBF or polynomial). It is powerful for high-dimensional datasets and provides good accuracy.\n",
        "\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        " = The Kernel Trick in SVM allows it to handle non-linear data by transforming input data into a higher-dimensional space without explicitly computing the transformation. This helps the SVM find a linear separator in that higher space, which corresponds to a non-linear boundary in the original space. Common kernels include linear, polynomial, and RBF (Radial Basis Function). It makes SVMs powerful for complex, non-linear classification problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "PoSEaPG3MQFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_pred = svm_linear.predict(X_test)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_pred = svm_rbf.predict(X_test)\n",
        "\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(\"Accuracy with Linear Kernel:\", linear_acc)\n",
        "print(\"Accuracy with RBF Kernel:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tOfLin9Mndf",
        "outputId": "475e1488-c0f8-4a6f-df8a-33ea630b12bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9814814814814815\n",
            "Accuracy with RBF Kernel: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "= The Na√Øve Bayes classifier is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem, used mainly for classification tasks. It calculates the probability of each class given the input features and chooses the class with the highest probability. It is called ‚ÄúNa√Øve‚Äù because it assumes that all features are independent of each other ‚Äî an assumption that is rarely true in real-world data but still works well in practice, especially for text classification and spam detection.\n",
        "\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "- Gaussian Na√Øve Bayes: Used for continuous data that follows a normal (Gaussian) distribution. Example ‚Äì predicting based on height, weight, or age.\n",
        "- Multinomial Na√Øve Bayes: Used for discrete count data, like word frequencies in text classification (e.g., spam detection).\n",
        "- Bernoulli Na√Øve Bayes: Used for binary/boolean features (0 or 1), indicating the presence or absence of a feature.\n",
        "- Key Difference: The type of data each model handles ‚Äî continuous (Gaussian), counts (Multinomial), or binary (Bernoulli).\n",
        "- Common Use Case: Gaussian for numeric data, Multinomial/Bernoulli for text data."
      ],
      "metadata": {
        "id": "Oe4GNlJGM2Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Na√Øve Bayes:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njt-2J1UNRsn",
        "outputId": "6ebc4611-ccef-4b8f-e5cc-3a0ad9d19d44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Na√Øve Bayes: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}